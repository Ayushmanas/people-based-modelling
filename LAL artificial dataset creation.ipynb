{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e68415d",
   "metadata": {},
   "source": [
    "### Look A Like Modelling Dataset creation (dont need this in real world)\n",
    "\n",
    "Hello all. Since this is beginning of the one of the people based modelling processes, LAL modelling, we will be looking into\n",
    "creating a seed data, and a overall data. With Look a likes, we usually have big datasets consisting of unique users who may or may not have the same behavior which are defined over a particular list of variables (or features), which describe the user over some dimensions.\n",
    "\n",
    "The goal of a look a like model is to find/ score a populus based on the maximum likelihood of behaviors they have over a seed data, something we want to find the users against.\n",
    "\n",
    "This seed data are usually identifiers of a certian set of people, whose behaviors we are trying to map over certain behaviors or / dimensions and look for people with the most likelihood. The seed usually comes from a different analysis, or part of some sales / reach / leads / etc, independent to the behavior at first, we are tryinh to map it back to. This is usually part of a campaign data. Seed represents only a list of representatives / population, and some way to join it with our data. It is a small sample. \n",
    "\n",
    "However, the seed data is usualy restricted in terms of size and is ideally a subset of some other group of users. In a more realistic scenario, it could be for e.g., buyers for a car over a period which purchased over our marketing campaign, while the base could be of people who have showed intrest in buying our car but never bought the car. ( or could be an entire population of a locality)\n",
    "\n",
    "We will then look into people who could have been the maximum likelihood of being potiential buyers of this car, what their most prominent features are, as well a decile (or any subset) or groups, which are the closest and will be targeted in the next ad-campaigns over any targeting platform. This helps in reducing costs for targeting (this is expensive you know, since it pays per user) as well as testing out any test control groups for any campaign analysis and with reporting. This could also be a prt of A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be0786",
   "metadata": {},
   "source": [
    "Any real world data could be purchased from Vendors, and usually go through a lot of preprocess to get us this info, e.g., data cleaning, removal of PII, mapping data to an hashed Identifier ID, hashing / creating dummy variables for x feilds, etc. This also includes getting it verified with a law unit in your org or through a data governacnce team.\n",
    "\n",
    "##### Again, since the actual real world data cannot be obtained easily and could be very risky to deal with without governance ;-( , I will try to create an artificial dataset, something which closely resembles the real world collective data and seed so we can begin our analysis\n",
    "\n",
    "Right now, we are doing the whole thing in python, but once i get to kno how to install and operate pyspark on local machine I might try that version too, as that is the ideally what any data science company likes to operate it on. Remember, python is non distributed single node working machine may not handle any big data of size of trillions(welcome to real world data!) efficiently.\n",
    "\n",
    "However, lets get started !\n",
    "\n",
    "We will be using the python's blob data function to create a dataset of size 30k, while keeping the seed to be 1.5k in size.\n",
    "The number is features should be big tho(again following real world metadata you will usually deal with), like 150.\n",
    "\n",
    "All the best! :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c7ee6",
   "metadata": {},
   "source": [
    "###### Note : This script is supposed to be run once, and it will save the data in your local. DO NOT RE-RUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00895b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_blobs(n_samples=30000, n_features=80, cluster_std=20.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7df247",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].shape # this is your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398528ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for this dataset taken straight from chat GPT (I am out of ideas, sue me)\n",
    "# On second thought -  please dont :'(\n",
    "\n",
    "# Since features are important for our analysis, and in terms of handling model explainability, we will be keeping feature names as it is\n",
    "# and not do any PCA's. (Important)\n",
    "\n",
    "features=[('Feature_'+'0'*(3-len(str(i)))+str(i)) for i in range(1,81)]\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifiers for users, since they are lawfully restricted not to use / share / work with any PII information in raw form, or \n",
    "# as anything closely resembling it. (usually the case)\n",
    "# Here I am generating a simpler artificial ids:\n",
    "identifer_ids = [('INT'+'0'*(5-len(str(i)))+str(i)) for i in range(1,30001)]\n",
    "\n",
    "\n",
    "# Create pandas data frame\n",
    "df_univ=pd.DataFrame(data=data[0], columns=features, index=identifer_ids)\n",
    "df_univ.insert(loc=0, column='User_ID' ,value=df_univ.index)\n",
    "df_univ.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_univ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b78f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I am thinking of this data set as video watched hours for several shows / timings / recent watch behavior, etc. so keep it positive\n",
    "df_univ[features]=df_univ[features].apply(lambda x:np.abs(x)*10)\n",
    "df_univ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_univ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_univ.to_csv('movie_universe_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7980a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking a stratified sample this time\n",
    "seed_df = df_univ.apply(lambda r: sum([r[i] for i in features]) , axis=1)\n",
    "seed_df.index=df_univ['User_ID']\n",
    "seed_df = seed_df.sort_values(ascending=False)\n",
    "\n",
    "## Take seed from 85 percentile\n",
    "qntl = seed_df.quantile(.85)\n",
    "seed_df = seed_df[seed_df>qntl]\n",
    "\n",
    "if seed_df.shape[0]>1500:\n",
    "    seed_df=seed_df.sample(n=1500, random_state=123,)\n",
    "    \n",
    "seed_df['User_ID'] = seed_df.index \n",
    "seed_df=pd.DataFrame(seed_df['User_ID']).reset_index(drop=True)\n",
    "print(seed_df.shape)\n",
    "print(df_univ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether they can join or not\n",
    "# .join works for joining by indexes, so leave that, and use merge. works the same\n",
    "temp=pd.merge(seed_df, df_univ, how='left', on='User_ID')\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the seed IDs\n",
    "#temp.head()\n",
    "temp=None\n",
    "seed_df.to_csv('seed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76632fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b1f03-5ef2-4f31-acff-33d83a2ab6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "seed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b50e3-92a5-4163-9ccd-e2715fdcb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d3a31-8b27-4651-a232-7f889bf8913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ydata_profiling import ProfileReport\n",
    "import sweetviz as sv\n",
    "\n",
    "#report = ProfileReport(df_univ, title='EDA of dataset')\n",
    "report = sv.analyze(df_univ)\n",
    "\n",
    "report.show_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd70742-2005-4b02-964c-567aba38fa03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ca32e-eaaa-4a77-ba76-19de73e27119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
